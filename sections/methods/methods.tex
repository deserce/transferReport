\chapter{Computational methods}
\section{Introduction}
Computational techniques are a vital tool in the prediction and characterisation of a wide range of solid state materials.
Not only can computational methods aid synthesis strategies by identifying promising material candidates in screening studies, but they can offer atomic scale insights into the structural and transport properties of materials not readily observed by experiment alone.
This study primarily uses potentials-based energy minimisation and Molecular Dynamics (MD)  techniques implemented in GULP \cite{Gale2003} and LAMMPS \cite{StevePlimton1995} respectively.
In this chapter, the fundamental concepts and mathematics underpinning these techniques are presented.
Further to this, conventional techniques for parameter derivation are contrasted with techniques currently in development.
This overview is brief and focuses primarily on techniques used in this report, as more comprehensive reviews are available elsewhere. \cite{Gale2003, Jensen2007, Catlow2013}
Some work presented in this section is derived from the MRes report which led to this project, as the fundamental equations underpinning computational methods remain the same.

\section{Atomistic modelling}
An understanding of the forces atoms experience as a function of their environment is vital to gain insights into the properties of systems of interest.
One class of approaches for calculating the interatomic forces between atoms, \textit{ab initio} techniques, are based on quantum mechanics, and seek to determine electron density as a function of position. Whilst the scale of systems these strategies can be used for is increasing with developments in computational resource available, these techniques remain prohibitively expensive for large systems or systems studied over large time-scales.
In contrast, atomistic techniques seek to express the forces between atoms as a series of empirically derived models, either fitted to experimental or \textit{ab initio} data, which can be cheaply evaluated, allowing for large scale systems to be studied over large time scales. 
This is particularly important when considering phenomena which occur over large time scales, such as diffusion. 

\section{Potential models}
All atoms in a system, and their current position, momentum, and oxidation state, give rise to highly complex interactions contributing to the internal energy of the system, which must be enumerated if a perfect atomistic model is to be developed.
Potential models are used to determine the forces in such systems by treating these as a series of discrete isolated interactions which sum to the same overall effect. 
The internal energy of the system can then be given as the sum from one to n-body terms as such:\cite{Gale2003}
\begin{align}
U &=& &\sum_{i = 1} U_i&         &+& &\frac{1}{2}\sum_{i = 1} \sum_{j = 1} U_{ij}&  &+& &\frac{1}{6}\sum_{i = 1} \sum_{j = 1} \sum_{k = 1} U_{ijk}& &+& &\cdots\\
&=& &\sum_{i = 1}^\prime U_i&  &+& &\sum_{i,j = 1}^\prime U_{ij}&                 &+& &\sum_{i,j,k = 1}^\prime U_{ijk}&                           &+& &\cdots
\label{eq:terms}
\end{align}
With the first term accounting for single-body terms, the second term accounting for those interactions which exist between pairs of bodies and so on.
The primed sum in Equation \ref{eq:terms} distinguishes between interactions that are enumerated repeatedly, and instead counts each unique interaction only once, hence cancelling the $\frac{1}{N!}$ term.
This expansion is exact if expanded to account for all N-body interactions in a system.

As it is only the low order terms which give rise to the majority of the internal energy of a system, it suffices to truncate this expansion to lower order terms.
Typically, this is limited to two-body terms, although higher order terms are included where these strongly impact the system (e.g. bond angles).
Beyond this, higher order terms are only considered where the calculation of a specific property of interest requires their inclusion.
Three-body interactions are required to calculate phonon distribution curves for example.
Furthermore, as the number of bodies in an interaction increases, it becomes increasingly difficult to find a rational physical basis on which an empirical model can be based. 
 
Single body interactions are implemented to represent an external forcefield being applied to a system, such as the application of an external electric field interacting with charged species.
They are also used in the Einstein model, in which species do not interact with one another, and instead are attached to their lattice sites by a spring term.
Single body terms are not used in the course of this study, but are referenced here for completeness sake.
\subsection{Two-body interactions}
The two-body terms in Equation \ref{eq:terms}: 
\begin{equation}
\sum_{i,j = 1}^\prime U_{ij}
\end{equation}
account for the interactions occurring between pairs of atoms or ions in isolation.
As no reference frame is given, these terms are therefore strictly a function of interatomic separation.
It is conventional to further subdivide these two-body interactions into Coulombic (long-range) and short-range terms as so:
\begin{equation}
U_{ij} = \Phi_{Coulombic} + \Phi_{short-range}
\label{eq:two-body}
\end{equation}

The Coulombic term in Equation \ref{eq:two-body} is the potential arising from electrostatic interactions between pairs of charged species:

\begin{equation}
\Phi_{Coulombic} = \frac{q_iq_j}{r_{ij}}
\label{eq:coulombic}
\end{equation}
Where $q_i$ and $q_j$ are the effective charges of the species.
This term is colloquially referred to as the ``long-range'' term, as the non-Coulombic component of these interactions are comparatively tiny beyond a certain range.
It is also typically the dominant component in a system at equilibrium, accounting for around 90\% of the total potential energy in a usual system.\cite{Catlow2013}

The long-range nature of the Coulombic term leads to slow convergence in direct space for a large number of ions.
The \citet{Ewald1921} summation can be used to overcome this shortcoming in periodic systems, by further subdividing the Coulombic interactions into short- and long-range terms.
The short-range components are computed as before, while the long-range terms are solved in reciprocal space.

The nature of a two-body interaction can vary widely dependent on context, with the species, charge, and even local environment impacting the resultant potential.
Depending on the system, covalent interactions, London interactions and electron-pair repulsions may all need to be accounted for with no actual calculation of electron density occurring on which to base the potential.
Instead, the resultant force is generally calculated using an empirical model derived to match experimental or \textit{ab initio} data.

The selection of an appropriate model is vital, with considerations including the ionic/covalent nature of the system, availability of experimental data, and computational cost influencing the choice of model.
Furthermore, given potential models are often derived to match experimental data for a given system, ensuring that that same potential is suitable when applied outside of the context in which it was developed is important.

This of course can pose an issue when seeking to study systems where no experimental data is available with which to verify the findings of computational work.
 For ionic systems, the Buckingham Potential form is commonly used for two-body interactions:


\begin{equation}
\Phi_{ij} = A\cdot \exp \left(\frac{-r_{ij}}{\rho_{ij}} \right) - \frac{C}{r_{ij}^6}
\label{eq:Buckingham}
\end{equation}

\noindent Other common short range potential models are given in Table \ref{tab:potentialmodels}

\newpage

\begin{table}[t]
  \centering
  \caption{Common short-range two-body potential models.\cite{Gale2003}}
  \label{tab:potentialmodels}
  \begin{tabular}{@{}lc@{}}
  \toprule
  Potential Model         & Expression     \\
  \midrule
  Buckingham              & $\displaystyle{\Phi_{ij} = A\cdot \exp \left(\frac{-r_{ij}}{\rho_{ij}} \right) - \frac{C}{r_{ij}^6}}$    \\
  \addlinespace
  Harmonic                & $\displaystyle{\Phi_{ij} = \frac{1}{2}k_2(r_{ij}-r_0)^2   + \frac{1}{6}k_3(r_{ij}-r_0)^3    + \frac{1}{24}k_4(r_{ij}-r_0)^4   }$    \\
  \addlinespace
  Lennard-Jones           & $\displaystyle{\Phi_{ij} = \left(\frac{A}{r_{ij}^m} \right) - \left( \frac{B}{r_{ij}^n}\right)}$   \\
  \addlinespace
  Morse                   & $\displaystyle{\Phi_{ij} = D_e \left((1- \exp(-a(r_{ij} - r_0)))^2           -1\right) }$    \\
  \bottomrule
  \end{tabular}
\end{table}

\vspace{-5pt}

\paragraph{Finite-range implementation}
As the name suggests, the short-range terms dominate in the short-range, but the force arising from these interactions decreases rapidly with increasing interatomic distance.
To decrease computational expense, a threshold radius is defined beyond which all forces are assumed to be zero.
The selection of this threshold is important, as the computational expense is of $O(r_t^2)$, so the shorter this radius the less expensive the simulation, whereas too small a radius will result in a change in the simulation result and decrease in model quality.
In order to prevent this strategy introducing discontinuity into the system, additional terms are added to smoothly reduce the interatomic potential and associated energy derivatives to zero at the threshold radius. 


\subsection{Three-body interactions}
Three-body interactions can be interpreted as those which arise due to bond/electron pair repulsion or charge dispersion between three bodies in covalent and ionic models of bonding respectively.
The inclusion of this term is not necessary in all systems, but can increase the quality of certain battery models, particularly when modelling polyanion species, such as \ce{PO4} and \ce{SiO4}.
The magnitude of this term is relatively small compared to two-body interactions, and unless a strong three-body interaction exists in a system, it is only included to calculate of properties dependent on three-body interactions such as phonon dispersion curves.

A commonly used three-body potential is the harmonic model:
\begin{equation}
  U_{ijk} = \frac{1}{2}k_2(\theta-\theta_0)^2   + \frac{1}{6}k_3(\theta-\theta_0)^3    + \frac{1}{24}k_4(\theta-\theta_0)^4
  \label{eq:threebody}
\end{equation}

In this model, an equilibrium angle ($\Theta_0$) is assigned to a bond pair, and any deviations from this angle increase the potential energy of the system.

\subsection{Polarisability}
The electron distribution surrounding an atom or ion will shift according to the local environment in which it exists, effectively allowing the internuclear and electronic interactions to partially decouple.
A rigid ion model, in which species are modelled as point charges, does not capture this behaviour, decreasing the accuracy of models using it.
This is a particularly important behaviour when studying ion migration, and so a failure to account for this behaviour can negatively impact the accuracy of results obtained.

An early model of ion polarisation is the point polarisable ion model (PPI), in which the dipole moment of an ion ($\mu$) is directly proportional to the strength of the electric field in which it sits (E):
\begin{equation}
\mu = \alpha E
\end{equation}
Whilst computationally inexpensive and easily extensible to higher order polarisabilities (i.e. quadrupolar systems), this model performs poorly when handling dynamic lattice properties.
It also poorly predicts dielectric constants, which is easily measurable experimentally and thus a useful criteria for model validation.

This shortcoming can be rationalised by the failure to account for the polarisation of adjacent ions, that is, that neighbouring species being polarised impacts the local electric field and serves to dampen polarisation overall.
Consequently, this model tends to overestimate dielectric constants if tuned to accurately predict another physical property (e.g. elastic constants).


\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figures/coreshell/coreshell}
  \caption[Shell model schematic]{Schematic of the shell model of polarisability. The valence electron shells (grey) are attached to the ion core by a harmonic spring, and are displaced by nearby charged species or net electric fields.}
\end{figure}
The shell model\cite{Dick1958} is another computationally inexpensive model of polarisation which accounts for polarisation coupling, thus overcoming a key limitation of the PPI model.
Polarisation coupling is modelled by treating the valence electron cloud as a shell connected to an ionic core by a harmonic spring.
The core represents the nucleus and core electrons of the ion, accounting for the majority of the mass of the ion, while the shell is represents the polarisable valence electrons.
The mass of the shell is treated as zero in non-dynamical systems (i.e. in structural optimisation, Section \ref{sec:minimization}) where the solver strategies are able to handle zero-mass species without instability.
In the case of dynamic systems (i.e. Molecular dynamics, Section \ref{sec:MD}), the shell is assigned a small portion of the overall mass to provide a degree of inertia and remove the need for prohibitively small time-steps.

By allowing the displacement of valence electrons from the ion core, an effective dampening of the polarisation occurs, offering better agreement with experiment.

The charges assigned to the core and shell respectively must sum to that of the point charge it replaces, but the values of these and the spring constant are typically empirically derived.
This polarisation model generally gives good agreement with experiment for ionic halides and oxides.

\section{Partial occupancy and disorder}
Many materials have complex structures exhibiting some form of disorder.
Partial occupancy, a common form of disorder, is present in systems where there are more symmetry degenerate sites than there are species to occupy them, or where a number of ions occupy the same structural position.\cite{Gale2003}
In principle, the only way to accurately model such systems is to generate supercells of the material of a given size with the correct stoichiometry.
Properties can be calculated for each supercell, and the properties of the overall material can be expressed as a weighted average of these simulations.
Whilst useful in some cases, the huge number of potential supercells for highly disordered materials can make this approach infeasible.
A simple example of distributing 50 ions and 50 vacancies across 100 symmetry inequivalent sites yields $\frac{100!}{50!\cdot50!} = 10^{29}$ distinct cases.
Randomly generating supercells in a none-exhaustive study is far cheaper, but the high formation energy of some local environments is not accounted for in such a strategy, leading to a disproportionately high number of non-favourable environments forming.

Another strategy for studying disordered systems is to use the mean-field approximation. 
At each partially occupied site, the interatomic potentials are calculated as usual for each species which occupies the site. 
The forces are then scaled according to the relative occupancy of each species.
This technique can be powerful for initial studies of systems in that it yields symmetric (and thus robust) systems.
It is particularly useful for the calculation of structural properties, where the averaging of forces arising from each species effectively occurs across the bulk.
The simplification of lattice sites fails to capture the nuance of local environments, potentially missing properties that might only arise in certain configurations.

A final strategy is to approximate the disordered solid as being ordered.
A benefit of this strategy is it can be coupled with DFT studies to predict low energy ordered structures as a basis for the calculation.
This of course relies upon a means of generating a low energy or ground state structure for the system being studied, which is non-trivial for highly disordered systems.
One such strategy to find these structures, cluster expansion,\cite{Chang2019} may be implemented by the author in future work.


\section{Energy minimisation}
\label{sec:minimization}
\subsection{The configuration space}
\label{sec:config}
The internal energy of an ionic system is a function of its atomic coordinates.
While such systems are more easily thought of as a series of points in a 3-space, the resultant energy surface is $3n$ dimensional for a system containing $n$ atoms, with each point on this surface corresponding to a unique set of atomic coordinates.
We define the vector of positions of these atoms, the configuration space, $\mathbf{x}$.

Whilst each position in the configuration space has an associated energy, $U(\mathbf{x})$, real systems tend towards more energetically favourable states.
Stationary points on the energy surface, those where $\nabla U(\mathbf{x}) = 0$, are of particular interest:
\begin{labeling}{\textbf{Saddle point}}
	\item [\textbf{Minima}] Points on the energy surface where:
	\begin{equation}
	\nabla^2 U(\mathbf{x}) > 0
	\end{equation}
	\noindent
	These points correspond to stable structures.
	Multiple minima may exist, corresponding to different configurations or phases that the system can occupy, while the lowest energy minima, the global minima, corresponds to the ground state of the system.
	\item [\textbf{Saddle point}] The point corresponding to the maximum energy along the minimum energy path between two minima.
	These points correspond to transition states, and are of interest when studying phase changes or ion migration.
\end{labeling}

It is worth noting that whilst it is trivial to confirm that a point is stationary, it is not possible to establish whether a point is globally minumum without an exhaustive search,
Some techniques such as Monte-Carlo methods can, with sufficient run time, identify points which are most likely globally minimum, but for all but the simplest systems, a global search is infeasible.\cite{Barnes1992}


As local minimisation techniques yield the global minimum so long as initialised near the final value, good initial conditions derived from experiment, \textit{ab initio} studies, or an educated guess can obviate the need for global optimisers in some cases.

\subsection{Gradient-based methods}

The internal energy of a system at a given point on the energy surface can be expanded into a Taylor series:
\begin{equation}
\label{eq:taylor}
  U(\mathbf{x}+\delta \mathbf{x}) = U(\mathbf{x}) + \frac{\partial U}{\partial \mathbf{x}} \delta \mathbf{x} + \frac{1}{2!} \frac{\partial ^2 U}{\partial \mathbf{x}^2}(\delta \mathbf{x})^2 \cdots
\end{equation}

Truncating this equation at the second term yields the energy at a given point on the energy surface $U(\mathbf{x})$, and a derivative vector $g$, corresponding to the direction on the energy surface where $U(\mathbf{x})$ increases most rapidly.

\subsubsection{Steepest descent}
In this method, the position in the energy space $\mathbf{x}$ is iteratively updated by:
\begin{equation}
\mathbf{x}_{p+1} = \mathbf{x}_p + g^p\delta
\end{equation}

with an appropriate value of $\delta$ informed by line searches.
The procedure is repeated until some convergence criteria is met.
Whilst calculating the next step is relatively inexpensive, there is no theoretical limit to the number iterations required for convergence.
Furthermore, as for each $\mathbf{x}_p$, $U(\mathbf{x_p})$ must be evaluated, this can be an expensive procedure if evaluating $U(\mathbf{x_p})$ is costly, i.e. for systems with large numbers of species.
This technique also recalculates the gradient vector each step, making no use of information from prior iterations.

\subsubsection{Conjugate gradients}
The conjugate gradients method optimises in $g^0$ on the first iteration, running until a minimum along that vector is found.
$g^1$ is then evaluated and the minimisation procedure is repeated. 
Each new vector is constrained to being orthogonal to all previous search vectors.
If $\mathbf{x}_0$ is sufficiently close to a minima, the energy surface in the search region will be approximately quadratic. 
In this case, the conjugate gradients method converges in $N$ steps for an $N$ dimensional energy surface. 
Put another way, it converges in $3N$ steps for a $N$ body system in 3D space.

\subsubsection{Newton-Raphson Method}
The Newton-Raphson method is a widely used second order minimiser.
Rather than truncating the Taylor expansion of the energy surface (Equation \ref{eq:taylor}) at the first derivative, as in the steepest descent and conjugate gradient methods, an additional second derivative term is included (yielding a Hessian matrix $H$).


\begin{equation}
\Delta \mathbf{x}_p = -H^{-1}g^p
\end{equation}
\begin{equation}
\mathbf{x}_{p+1} = \mathbf{x}_p + \Delta \mathbf{x}_p
\end{equation}

As with the conjugate gradients method, an initial condition near the minima will guarantee convergence, in this case in a single iteration.
That said, outside the quadratic region near minima, the Newton-Raphson method can be numerically unstable.
Furthermore, calculating and inverting the Hessian matrix is a highly expensive.
As such, a BFGS optimiser\cite{Shanno1970} is typically used to update the Hessian between iterations, only explicitly recalculating the Hessian when some criterion indicating the updated Hessian is likely inaccurate is met.
This is considerably cheaper than a Newton-Raphson method on its own.

Where numerical stability is an issue, conjugate gradients or a steepest descent can be used for an initial pass until close to the minima.
The BFGS optimiser can then be used once in a more stable region of the configuration space.

As it is possible to run optimisers until converged to an arbitrary degree of precision (far beyond chemical accuracy), sensible convergence criteria are required to prevent excessively long calculations.
Unless otherwise stated, the default convergence criteria in GULP and LAMMPS are used in this report, with any results presented having converged to at least the degree of precision reported.

\newpage
\section{Periodic boundary conditions}
\begin{figure}[hb]
  \centering
  \includegraphics[width = 0.4\linewidth]{figures/pbc/pbc}
  \caption[Periodic boundary conditions schematic]{Schematic of a simulation using periodic boundary conditions.}
  \label{fig:periodic}
\end{figure}
At the scale of atomistic modelling, the materials being studied effectively stretch out infinitely relative to the scale of short-range and Coulombic interactions.
It is of course not feasible to model these systems as such, so boundary conditions which closely approximate this are needed.

Periodic boundary conditions (Figure \ref{fig:periodic}) take the contents of the simulation, and create copies of it at each boundary.
When calculating interactions, species in the ``main'' simulation are able to interact with other species in the main simulation, and those in the repeated simulations, subject to cutoff criteria applied to said interactions.
As the system is infinitely repeating, each copy of an atom must, by definition, be experiencing the same net force as each of its copies.
As such, it is sufficient to calculate the forces on bodies in a single cell only.

Another important feature of periodic boundary conditions is that atoms which would leave the boundaries of the simulation in the next iteration or time-step are reintroduced at the opposite boundary.
For molecular dynamics simulations (Section \ref{sec:MD}), this allows for atoms to move freely rather than being constrained to certain positions in order to keep a constant number of bodies in the simulation.
This is particularly useful when studying diffusion, allowing atoms near simulation boundaries to hop to all adjacent sites.


\newpage


\section{Point defects}
Whilst the atomistic modelling techniques already discussed are applicable in the generic case (as can be demonstrated by their suitability when applied to a P1 symmetry group), they have thus far only been discussed in the context of bulk systems.
In solid-state materials, it is often the presence of defects which gives rise to properties of interest, most likely in concentrations far too low to be studied in a periodic system.
As an example, the energy associated with the formation and migration of \ce{Li+} ions in candidate electrode materials is a key metric of their likely performance.

An overview of defect types, as well as the formalisms used in defining defect formation, is given in the appendix.

\subsection{Mott-Littleton method}
The Mott-Littleton method allows for the modelling of one or more defects at infinite dilution.
The perfect crystal structure is first solved using the techniques listed in Section \ref{sec:minimization}, and the converged result used as an initial condition.
Defects are then introduced to the system, and it is once more allowed to relax.
The difference in energy of the two systems is directly attributable to the perturbation introduced, and is therefore the energy required to form the defect introduced.
Use of a small periodic system would repeat the defect and be equivalent to simulating the defect at high concentration.

Instead, the system is divided into three concentric spherical regions, as illustrated in Figure \ref{fig:mott}.
As the introduction of a single point defect removes symmetry from the system, a periodic approach is not appropriate, nor is the consideration of an infinite system.

\begin{figure}
  \centering
  \includegraphics[width = \linewidth]{figures/mott/mott}
  \caption[Mott-Littleton method schematic]{Schematic illustrating the Mott-Littleton method. Ions in region I (green) are modelled explicitly using potential forms due to their proximity to the defect (square). Region IIb ions (white) are modelled using a continuum method. Region IIa ions (green) use a harmonic relationship, smoothing the transition from explicit (Region I) to implicit (Region IIb) modelling.}
  \label{fig:mott}
\end{figure}

Region I contains those ions closest to the defect.
Their close proximity to the defect means no simple approximation can be made to predict the defects impact.
As such, the relaxation of these species is modelled explicitly, using potential models and gradient descent techniques.

Ions in Region II are sufficiently far from the defect to be reasonably approximated using analytical methods.
Region IIb, the outermost region, is sufficiently far from the defect to only impacted by Coulombic interactions with the defect.
These are modelled using a dielectric continuum, the magnitude of the force experienced, and corresponding energy change, being a function of the net charge of the defect and the distance from the defect centre.


Ions in region IIa are modelled as having harmonic interactions with the defect, having an energy cost associated with their displacement from their initial position.
This region smooths the transition between the explicit modelling regime in region I and the implicit modelling regime in region IIb, avoiding discontinuities in the resultant energy surface.

The success of this technique relies upon selection of radii for the regions such that discontinuities do not occur between regions.
Selection of radii intervals greater than the cut-off values imposed on short range interatomic potentials typically achieves this.
That is to say, region IIa should be large enough such that no species in region I have any short range interactions with species in region IIb.

Further, the computational cost of relaxing region I is approximately of $O(r^2!)$.
As such, a radius for region I should be chosen such that the resultant defect energy does not change with increasing radius, whilst being as small as possible to reduce simulation time.

\subsection{Supercell approach}  
Supercell methods simply take a large supercell of the repeating crystal unit and add a defect to this supercell.
The cell is then modelled as before, with the supercell repeating ``infinitely''.
This approach is limited in that in order to model low defect concentrations, a huge supercell is required.

As such, this method is generally instead only applied in Molecular Dynamics (Section \ref{sec:MD}), where a large concentration of Li vacancies are added in order to allow diffusion events to be observed.

\subsubsection{Symmetry optimisation}
Space groups allow for crystal structures to be expressed with their inherent symmetry included.
These symmetrical elements will often lead to a number of interactions being equivalent.
This in turn leads either to situations where the number of evaluations of potential functions can be reduced, or forces the evaluation of a force can be disregarded where it is known that the forces will always cancel each other out.
GULP will automatically use symmetry optimised minimisation techniques where possible.


\section{Molecular dynamics (MD)}
\label{sec:MD}
Energy minimisation techniques are used to study systems at zero Kelvin.
As such, phenomenon arising only where ions have thermal energy cannot be studied with these techniques.
Molecular dynamics (MD) can overcome this issue by assigning each particle a velocity (usually subject to a Boltzmann distribution) and allows the system to evolve over time subject to some imposed constraints.
This in effect allows for phenomenon arising from thermal effects to be studied,
assigns and tracks kinetic energy of atoms, allowing the system to evolve over time.

The equation which fundamentally governs MD simulations is simply Newtons law of motion:
\begin{equation}
	\mathbf{F} = m\mathbf{a}
\end{equation}

As the force each particle is a function of the positions of each particle in space, which in turn evolves as a function of time, it is necessary to solve this equation repeatedly.
The force acting on each particle is assumed to be constant for some small period of time $\Delta t$.
With this assumption, a number of integration schemes can be used to predict the state of the system at time $t +\Delta t$.
\newpage
\subsection{Integration schemes}
The Taylor expansion is a technique which allows a function to be expressed as a infinite sum of terms calculated from that functions derivatives at a given point:
\begin{equation}
f(t) = f(t_0 + \Delta t) = \sum_{n=0}^\infty \frac{1}{n!}f^{(n)}(t_0)\Delta t^n
\end{equation}
Applying this to the position of atoms in the system as a function of time yields:
\begin{align}
\mathbf{x}(t_0 + \Delta t) &= &\mathbf{x}(t_0) \; &+ &\mathbf{x}'(t_0)\Delta t \;&+& &\frac{1}{2}\mathbf{x}''(t_0)\Delta t^2 \;&+& \frac{1}{6}\mathbf{x}'''(t_0)\Delta t^3& +\;\dots\\
                           &= &\mathbf{x}_{t_0} \; &+  &\mathbf{v}_{t_0}\Delta t \;&+&   &\frac{1}{2}\mathbf{a}_{t_0}\Delta t^2     \;&+& \frac{1}{6}\mathbf{j}_{t_0}\Delta t^3& +\;\dots
\end{align}
Truncating this expansion at different terms yields a series of potential equations which can be solved, the discarded terms constituting the error in each step.
The lowest order discarded term will determine the magnitude of the error, with ``simpler'' expansions requiring a correspondingly smaller $\Delta t$ in order to reduce the magnitude of the error to an acceptable value.

\paragraph{Euler integration}
Truncating the Taylor expansion of the position and velocity of particles as a function of time at the first term gives an Euler integration scheme:
\begin{equation}
	\mathbf{v}_{t +\Delta t} = \mathbf{v}_t + \frac{d\mathbf{v}}{dt}\Delta t = \mathbf{v}_t + \mathbf{a}_t\Delta t +\Theta(\Delta t^2)
	\end{equation}
\begin{equation}
	\mathbf{x}_{t +\Delta t} = \mathbf{x}_t + \frac{d\mathbf{x}}{dt}\Delta t = \mathbf{x}_t + \mathbf{v}_t\Delta t +\Theta(\Delta t^2)
\end{equation}
With $\Theta$ representing the error, the brackets indicating the relationship between the size of the error and the choice of time step.
Unfortunately, the $\Theta(\Delta t^2)$ error terms in this integration scheme require a prohibitively small $\Delta t$ to be used for all but the simplest of systems. 

\paragraph{Verlet algorithm} A commonly used integration scheme, the Verlet algorithm, instead truncates the Taylor expansion of the atom positions at the third derivative:
\begin{equation}
\mathbf{x}_{t + \Delta t} = \mathbf{x}_{t} \; +  \mathbf{v}_{t}\Delta t \;+ \frac{1}{2}\mathbf{a}_{t}\Delta t^2     \;+ \frac{1}{6}\mathbf{j}_{t}\Delta t^3 + \Theta(\Delta t^4)
\label{eq:verlet1}
\end{equation}
with $j$ indicating the ``jerk'', or rate of change of acceleration of the function.
As the rate of change of acceleration is related to the rate of change of force, the jerk term is not readily calculated.
The Verlet algorithm overcomes this issue by using information known about the system in previous time steps $t_{t}$:
\begin{equation}
\mathbf{x}_{t - \Delta t} = \mathbf{x}_{t} \; -  \mathbf{v}_{t}\Delta t \;+ \frac{1}{2}\mathbf{a}_{t}\Delta t^2     \;- \frac{1}{6}\mathbf{j}_{t}\Delta t^3 + \Theta(\Delta t^4)
\label{eq:verlet2}
\end{equation}

Summing equations \ref{eq:verlet1} and \ref{eq:verlet2} yields:
\begin{align}
\mathbf{x}_{t + \Delta t} + \mathbf{x}_{t - \Delta t} &= 2\mathbf{x}_{t} + \mathbf{a}_{t}\Delta t^2 + \Theta(\Delta t^4)\\
\mathbf{x}_{t + \Delta t} &= 2\mathbf{x}_{t} + \mathbf{a}_{t}\Delta t^2 -  \mathbf{x}_{t - \Delta t} + \Theta(\Delta t^4)
\label{eq:verlet}
\end{align}

\noindent allowing for the positions of particles to be calculated at the next time step.
Interestingly, this integration scheme not only obviates determining the rate of change of acceleration of particles, it also skips the calculation of the velocity term.
The Verlet integrator is also time reversible and symplectic (preserves energy), making it suitable for the dynamic systems encountered in MD.
Of course, in order to calculate the kinetic energy of the system, knowledge of particle velocities is needed.
Subtraction of equation \ref{eq:verlet2} from \ref{eq:verlet2} yields:
\begin{equation}
	\mathbf{v}_{t} = \frac{\mathbf{x}_{t+\Delta t} - \mathbf{x}_{t-\Delta t}}{2\Delta t} + \Theta(\Delta t^2)
\end{equation}
giving the Verlet velocity.
As with the $\Theta(\Delta t^2)$ term being too large for calculating the position with a Euler integration scheme, the error term here requires prohibitively small time steps for complex systems.
Velocities are to determine the temperature of the system, which in turn must be a conserved quantity in certain ensembles. %TODO

\paragraph{Velocity Verlet} The Velocity-Verlet algorithm explicitly calculates velocities at half time steps:
\begin{align}
	\mathbf{v}_{t +\frac{1}{2}\Delta t} &= \frac{\mathbf{x}_{t+\Delta t} - \mathbf{x}_t}{\Delta t}\\[10pt]
	&= \mathbf{v}_t + \frac{1}{2}\mathbf{a}_t\Delta t + \Theta(\Delta t^3)
\end{align}
This half-velocity, the predicted velocity at the time half way between the current and next time step, is then used to update the current positions of ions in the system as in a Euler scheme:
\begin{equation}
	x_{t+\Delta t} = x_t + v_{t +\frac{1}{2}\Delta t}\Delta t + \Theta(\Delta t^4)
\end{equation}

\noindent The velocity at the next time step is then calculated by:
\begin{equation}
	\mathbf{v}_{t + \Delta t} = \mathbf{v}_{t +\frac{1}{2}\Delta t} + \frac{1}{2}\mathbf{a}_{t+\Delta t}\Delta t + \Theta(\Delta t^3)
\end{equation}

\noindent Explicitly calculating the velocities in this manner maintains the order of the error in the particle positions, whilst increasing the order of the error in the velocity terms.

\subsection{Time-stepping and equilibration}
The choice of time step will strongly influence the output of an MD simulation. 
Selection of an excessively small $\Delta t$ will result in simulations being too expensive to observe phenomenon which occur over large time periods such as bulk diffusion.
Counter to this, too large a $\Delta t$ can allow for atoms to occupy unphysical states.
For example, as the Velocity Verlet algorithm only evaluates forces at $x_{t}$ and $x_{t +\Delta t}$,  a particle with low kinetic energy may skip over an large energy barrier present at $x_{t +\frac{1}{2}\Delta t}$ if allowed to move too great a distance between time steps.
Typically, selecting time steps an order of magnitude below the period of atomic vibrations (\SIrange{0.1}{1}{\femto\second}) prevents unphysical phenomenon at the least computational expense.

\subsection{Initialisation and equilibration}
Prior to useful information being obtained fron an MD simulation, the system must be initialised and allowed to equilibrate.
This is generally done in a number of steps:
\paragraph{Structure generation}
Phenomenon of interest in battery materials, such as Li ion diffusion, occur over large length scales and are studied using statistical techniques.
As such a supercell approach is generally used to provide the sufficient bodies and space for these phenomenon to be observed.
Relaxed structures from energy minimisation codes, using the same potential forms as are used in the MD simulation, are commonly used as the repeating unit here.
In disordered solids, this step is non-trivial, with the configuration space of a typical MD simulation far exceeding that explorable by \textit{ab initio} methods.
Techniques allowing for low energy local structures to be characterised and ``tiled'' may be a technique to allow pseudo-random structure generation; an interesting avenue for future work, but not implemented as of yet.

\paragraph{Introduction of defects}
For diffusion to be observed, a key phenomenon in battery materials, some mobile species vacancies must be introduced.
Mobile species can then hop to these vacant sites, allowing ionic motion to be characterised.
The location of these defects is randomly assigned in this study, where bulk phenomenon are of interest.
The introduction of these defects must be charge compensated.
As MD does not explicitly calculate electron densities, it is not possible to associate the charge compensation with a specific set of atoms.
Instead the charge is ``smeared'' across all species which could have changed oxidation state for a given vacancy concentration.
As an example, in the case of introducing 10\% Li vacancies to a \ce{LiCoO2} cathode, a charge of $+0.1$ per formula unit must be assigned to the Co ions, giving an overall oxidation state of \ce{Co^{3.1}+}.

A second energy minimisation must then be performed to allow the structure to relax locally around the vacancies introduced.

\paragraph{Initialisation}
None of the integration schemes discussed are ``self-starting'', that is to say, they are reliant on some initial velocities being known in order to be used.
For a system containing $N$ bodies, velocities will be randomly assigned to give the system a desired initial temperature subject to:
\begin{equation}
	\sum^N_{i=1}m_i\mathbf{v}_i^2 = 3Nk_bT
	\label{eq:boltz}
\end{equation}
whilst ensuring the system has no net momentum in any vector:
\begin{equation}
	\sum^N_{i=1}m_i\mathbf{v_i} = 0
\end{equation}

While these velocities ensure that the desired kinetic energy is present in the initial steps of the simulation, it does nothing to prevent the assignment of unphysical velocities.
In order to prevent this biasing the results obtained, the simulation is allowed to run until a pseudo-steady state is reached. 
The number of steps necessary to obtain this is strongly case dependent, but can usually be visually identified in the data and discarded manually if necessary.

\subsection{Ensembles}
An ensemble is the set of all accessible states a simulation might access given sufficient time, subject to some constraints.
Selection of appropriate constraints enables a series of simulations to probe a system of interest across a range of specified conditions.
\paragraph{Canonical (NVT) ensemble}
The canonical ensemble keeps constant the number of bodies (N), the volume (V) of the simulation cell, and the temperature of the simulation (calculated by Equation \ref{eq:boltz}).
Whilst the number of bodies and cell volume are readily preserved, the internal energy and thus temperature may change over time due to rounding errors in the integration method.

\paragraph{Isobaric-isothermal (NPT) ensemble}
The Isobaric-isothermal ensemble keeps constant the number of bodies (N), the pressure (P) of the simulation cell, and the temperature of the simulation (calculated by Equation \ref{eq:boltz}).
In this instance, both the pressure and temperature of the system need adjusting over time.

A naiv{\"e} approach to maintain temperature is to calculate the temperature at each time step, and scale the velocities of each particle accordingly.
This technique prevents any temporary deviations in temperature, which is an unrealistic constraints.

A more realistic approach is to use a thermostat, coupling the system to a heat sink at the target temperature.
The rate of change of instantaneous velocity is then given by:
\begin{equation}
\frac{dT_{t}}{dt} = \frac{1}{\tau}(T_{sink} - T_t)
\end{equation}
where $\tau$ is in effect a gain term for a proportional controller attached to the system.
Similarly, the system pressure can be controlled by scaling the atomic coordinates of each particle subject to:
\begin{equation}
\frac{dP_{t}}{dt} = \frac{1}{\tau}(P_{sink} - P_t)
\end{equation}
\newpage
\subsection{Analysis}
In order to extract useful information from MD simulations, analysis of the positions of atoms as a function of time must be undertaken.
A number of key parameters are calculated, from which interesting material properties can be derived.
\paragraph{Mean square displacement}
The mean square displacement (MSD) measures the distance each atom has moved from its position at a given time since equilibration.
The MSD for an $N$ body system at time $t$ is defined as:
\begin{equation}
	MSD_t = \langle (\mathbf{x}_t - \mathbf{x}_0)^2 \rangle = \frac{1}{N}\sum_{i=1}^N(x_{i, t = t} - x_{i, t = 0})^2
\end{equation}
The MSD can be calculated for the system as a whole, or for a specific subset of particles.
By calculating the diffusion coefficient associated with all the particles of a given species, the self-diffusion coefficient of that species, for d-dimensional diffusion, is then given by:
\begin{align}
	MSD_t &= 2dDt\\[5pt]
	D &= \frac{MSD_{t_2} - MSD_{t_1}}{2d(t_2 - t_1)}t
\end{align}
\noindent
The activation barrier associated with an average site hopping event can be found by plotting the natural logarithm of the diffusion coefficient versus the inverse temperature, giving insights not accessible in zero Kelvin calculations:
\begin{equation}
	\ln{D} = \frac{-E_a}{R}\cdot\frac{1}{T} + \ln{A}
\end{equation}

\paragraph{Time averaged densities}
Given the vast quantities of data obtained over a moderate length MD simulation, visualisation techniques which allow for features persistent across the entire simulation are particularly useful.
One such technique, time averaged density plots, discretises the simulation domain and calculates the probability that a given species will be found in each subdomain at any given time.
Visualising this data thresholding to include only those regions with a high probability of containing species of interest, allows for migration pathways between lattice sites to be identified.
\section{Potential fitting}
In order for simulations to provide valuable insights into the material being studies, the potential forms used closely approximate the true energy surface.
Some prior information about the system must be available to serve as the basis to which parameters are fit.
Crystallographic data provides atomic coordinates and unit cell parameters which, if replicated by a given set of potential, serve as a good metric of performance.
Dielectric constants, elastic constants and other easily measured parameters may also serve as a basis for fitting.
In the absence of experimental data, it is possible to fit directly to the potential energy surface yielded from \textit{ab initio} studies.



\subsection{Metrics}
In order to distinguish between sets of potential parameters, an appropriate metric must be selected.
Some commonly used metrics to compare the basis information and calculated structure are:
\begin{labeling}{\textbf{Atomic displacements}}
	\item [\textbf{Lattice parameters}] The length and angles of the unit cell basis vectors
	\item [\textbf{Atomic displacements}] Cartesian or fractional displacement of each atom
	\item [\textbf{Forces}] The magnitude and direction of the force on each atom
\end{labeling}
Some or all of these metrics can be used, depending on the quality and nature of the basis information.
A generic cost function can then be defined as:
\begin{equation}
	F = \sum_{n=1}^{N_{obs}}w_i(f_i^{obs} - f_i^{calc})^{n_i}
\end{equation}

Where $w_i$ is a weighting term, increasing the bias of the fit towards matching certain observables.
$n_i$ can be set as 1 to give a linear fit, where the cost function is simply the sum of the error of the observables, or 2, giving the sum square of errors which more heavily penalises large errors in a given observation.

\subsection{Search algorithms}
A number of pre-existing strategies for searching the parameter space (analogous to the configuration space discussed in Section \ref{sec:minimization}).
Many of the techniques discussed in that section can also be applied to this problem.
However, the high dimensionality of parameter spaces may mean that several minima may exist when fitting to sparse data like crystallographic data.
In such cases local optimisers, which must be initialised close to the global minima for it to be found, may not be appropriate.

Global optimisers, such as genetic algorithms and Monte-Carlo are suitable in such cases.
Pairing global optimisers (to broadly sample the parameter space), with local optimisers (to improve on the quality of parameters yielded by the global optimiser) is one way of overcoming the limits of local optimisers without the cost of pure global optimisers.
A limitation of all these approaches is that it can be difficult to distinguish between parameter sets which replicate the physics of the interactions, and those which are only suitable for replicating the basis data.
This is especially likely when fitting to experimental parameters, introducing some degree of random error to the basis data.

An approach currently in development by the author, in collaboration with Dr. Benjamin Morgan and Dr. Lucy Morgan, is to use a Bayesian approach (SMC-ABC\cite{Toni2009a}) to derive interatomic potentials.
Instead of yielding a single parameter set by gradient descent or similar, each parameter is modelled as being described by some initially broad statistical distribution.
Parameter sets are sampled from these prior distributions, and those which adequately replicate the basis data sets are used to update the prior distribution, biasing the new distribution to more frequently give parameter values close to those used in accepted trials.
Iteratively applying this procedure yields distributions for each parameter being fitted, which correspond not to values which are necessarily the global minimum, but those values which offer good agreement with the basis data with low sensitivity to the values of the other fitted parameters, yielding robust parameter sets.

